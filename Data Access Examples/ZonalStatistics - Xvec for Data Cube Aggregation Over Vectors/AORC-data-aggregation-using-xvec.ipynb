{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f5079a-9bc3-491f-a388-45e15a79d396",
   "metadata": {},
   "source": [
    "# Analysis of Record for Calibration Data Aggregation Using Xvec\n",
    "#### Xvec: Vector data cubes for Xarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7da081c-3c03-40f6-a52a-9fc15aad26bc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f7f7f7; padding: 10px; border-radius: 5px;\">\n",
    "    <p><strong>Author:</strong> \n",
    "    <ul>\n",
    "        <li>Irene Garousi-Nejad, <a href=\"mailto:igarousi@cuahsi.org\">igarousi@cuahsi.org</a></li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p><strong>Last Modified:</strong> 1/24/25</p>\n",
    "    <p><strong>Description:</strong> Vector data cubes, such as those enabled by <strong>GeoCube</strong> or <strong>Xvec</strong>, provide a powerful approach for performing spatial analysis of gridded water and climate-related datasets across spatial features like watersheds, administrative boundaries, and ecological regions. This notebook demonstrates an example of how xvec can be used to aggregate AORC data over several subbasins within the Great Salt Lake basin. The ability to directly interact with vector geometries while leveraging the computational efficiency of libraries like Dask enhances workflow scalability, making it feasible to analyze large, complex datasets. Both Xvec and GeoCube are Python libraries designed for spatial analysis, but they serve slightly different purposes. Xvec is generally recommended when working primarily with gridded datasets in xarray, especially for tasks requiring advanced vector operations or scalable zonal statistics. In contrast, GeoCube is ideal for workflows involving GeoPandas vector data, as it focuses on rasterizing vector data for integration with raster datasets. While GeoCube is not optimized for large-scale or parallel processing, xvec is scalable and leverages Dask to efficiently handle large datasets.</p>\n",
    "    <p><strong>Software Requirements:</strong> This notebook has been tested using Python v3.11.8 using the following library versions:\n",
    "    <blockquote>\n",
    "        <ul>\n",
    "            <li>cf_xarray==0.10.0</li>\n",
    "            <li>dask==2024.4.1</li>\n",
    "            <li>dask-geopandas==0.3.1</li>\n",
    "            <li>fsspec==2024.3.1</li>\n",
    "            <li>geopandas==0.14.3</li>\n",
    "            <li>numpy==1.26.4</li>\n",
    "            <li>pint-xarray==0.3</li>\n",
    "            <li>rioxarray==0.15.3</li>\n",
    "            <li>xarray==2024.3.0</li>\n",
    "            <li>xarray-datatree==0.0.14</li>\n",
    "            <li>xgcm==0.6.1</li>\n",
    "            <li>xvec==0.3.1</li>\n",
    "            <li>jupyter</li>\n",
    "        </ul>\n",
    "    </blockquote>\n",
    "    </p>\n",
    "    <p><strong>Supporting Files and Dependencies:</strong> \n",
    "        <ul>\n",
    "            <li>Environment specification (requirements.txt)</li>\n",
    "            <li>GSLSubbasins shapefile dataset (GSLSubbasins.*)</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "    <p><strong>References:</strong></p>\n",
    "    <ul>\n",
    "        <li><a href=\"https://r-spatial.org/r/2022/09/12/vdc.html\">https://r-spatial.org/r/2022/09/12/vdc.html</a></li>\n",
    "        <li><a href=\"https://xvec.readthedocs.io/en/latest/index.html\">https://xvec.readthedocs.io/en/latest/index.html</a></li>\n",
    "        <li><a href=\"https://corteva.github.io/geocube/stable/examples/zonal_statistics.html\">https://corteva.github.io/geocube/stable/examples/zonal_statistics.html</a></li>\n",
    "    </ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b050c4-0e97-4c1d-9001-c3824f521cc1",
   "metadata": {},
   "source": [
    "### 1. Prepare the Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762be35a-4500-4a79-b904-77517391417f",
   "metadata": {},
   "source": [
    "Use the following command to ensure that all dependencies are installed in your environment. Note, these library versions have been pinned and tested for `Python 3.11.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182457a-02ae-47c5-ae56-23ac4160b059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786a62f-33da-42ab-a3db-1eff270930a4",
   "metadata": {},
   "source": [
    "Import the libraries needed to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8af1d-0384-4b1a-bcc7-06387d4b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dask\n",
    "import numpy\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import xvec\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3f85c-49f5-4675-bf15-67d61ffef742",
   "metadata": {},
   "source": [
    "We'll use `dask` to parallelize our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a43826-0b21-4037-b330-0f482eb81031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(client.dashboard_link)\n",
    "except:    \n",
    "    # The client should be customized to your workstation resources.\n",
    "    # This is configured for a \"Large\" instance on ciroh.awi.2i2c.cloud\n",
    "    # client = Client()\n",
    "    client = Client(n_workers=12, memory_limit='10GB') \n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdac78-34c9-4b42-a077-ac132bc2e5fc",
   "metadata": {},
   "source": [
    "### 2. Access the AORC Precipitation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c9447-7790-4546-9965-bcc1e0d1efe6",
   "metadata": {},
   "source": [
    "We'll be working with [AORC v1.1 precipitation data](https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/index.html#CONUS/zarr/forcing/precip.zarr/), which is used in the National Water Model version 3. These data are publicly available as part of the NOAA National Water Model v3.0 Retrospective archive on AWS registry of open data. These data are available in the Zarr format which offers a convienent and efficient means for slicing and subsetting very large datasets using libraries such as xarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ffa80-e04d-4b58-baf9-c6b5e4f0d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_url = 's3://noaa-nwm-retrospective-3-0-pds'\n",
    "region = 'CONUS'\n",
    "variable = 'precip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54715be3-3742-4e05-b768-36c2e667522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the zarr store that we want\n",
    "s3path = f\"{bucket_url}/{region}/zarr/forcing/{variable}.zarr\"\n",
    "\n",
    "# load these data using xarray\n",
    "ds = xr.open_zarr(fsspec.get_mapper(s3path, anon=True), consolidated=True)\n",
    "\n",
    "# preview the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f036b4-f076-4aa1-aeff-3dcceaa01e67",
   "metadata": {},
   "source": [
    "This Dataset is indexed by `x` and `y` representing the spatial grid. When aggregating using `ds.xvec.zonal_stats`, we are replacing these two dimensions with a single one with `shapely` geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8ea70-dae4-493f-985d-216087a3ac46",
   "metadata": {},
   "source": [
    "Print the projection system of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5308f14-6f27-4a6e-9d51-7f39b06f5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.crs.grid_mapping_name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8755e-78a9-4eff-8cf5-39936cf7be86",
   "metadata": {},
   "source": [
    "### 3. Load the geometry data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f83f16-1f39-49e2-a629-a63db43db27c",
   "metadata": {},
   "source": [
    "Use `Geopandas` to load a watershed Shapefile that defines our area of interest and includes hydrological subbasins as a set of polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc8a9d-7f30-467f-ac6e-c00232dc5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = gpd.read_file('./GSLSubbasins.shp')\n",
    "shp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c951c-42fd-45f8-a610-902f2478ef11",
   "metadata": {},
   "source": [
    "Preview the polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4347e-d415-4425-81ce-63e019b4d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp.plot(facecolor='white', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb1fed-4187-4a28-ae4a-601e64dd21c1",
   "metadata": {},
   "source": [
    "Print the coordinate reference system of the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d61cf-eef7-415f-8234-5fda567dd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shp.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2be70-a4fe-495e-b127-482de201b33a",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 4px solid #007acc; background-color: #f9f9f9; padding: 10px; border-radius: 4px;\">\n",
    "<strong>Note:</strong> <code>Xvec</code> method does not require a rioxarray CRS attached to the object but requires a user to ensure that the data are using the same projection. \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e496f4f-3bbe-4a6f-85fb-19a7caa3249e",
   "metadata": {},
   "source": [
    "### 4. Aligning Data Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b6667-35e9-431e-88af-cb5924dc7dce",
   "metadata": {},
   "source": [
    "We will reproject the shapefile to match the coordinate reference system (CRS) of the xarray dataset. To do this, we need to ensure that the CRS in the xarray dataset is properly set. Currently, the CRS is stored as a string in the metadata (`ds.crs.attrs['esri_pe_string']`) but is not correctly applied, meaning spatial operations like resampling and reprojection will not work. By using `rioxarray`’s `set_crs()` method, we can apply the CRS to the dataset in a standard geospatial format. The next cell extracts the CRS information from the metadata and then writes it to the dataset to ensure it is fully set and ready for spatial operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85b55a-35d7-496f-9e40-aba7b6dd30f8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 4px solid #007acc; background-color: #f9f9f9; padding: 10px; border-radius: 4px;\">\n",
    "<strong>Note:</strong> This should only be run once. Running it a second time will cause an error because the CRS will no longer exist as metadata once it has been set and written to the dataset. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4240f7b-bb84-4ba3-ac90-b012c580924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rio.set_crs(ds.crs.attrs['esri_pe_string'])\n",
    "ds.rio.write_crs(inplace=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef7713-9a16-4bbd-9e2d-d58b9cc4733b",
   "metadata": {},
   "source": [
    "Assign the CRS of the xarray dataset to the shapefile and ensure it has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff08d3-3ecc-4018-969f-f990d63c6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = shp.to_crs(ds.rio.crs)\n",
    "print(shp.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1541c0-15c9-468f-b658-80ad1e386467",
   "metadata": {},
   "source": [
    "### 5. Zonal Aggregation of AORC Over Sub-basins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2b4f4-330d-4f61-a613-bd51746a6c56",
   "metadata": {},
   "source": [
    "We use zonal statistics, a common aggregation method that involves summarizing raster values within a geometry. `xvec.zonal_stats()` can be used for this purpose, as it preserves the structure and attributes of the original data cube (`ds`) while enabling indexing by polygons. The `xvec.zonal_stats()` function accepts a range of parameters, including both required and optional ones. The table below provides the description of these parameters.\n",
    "\n",
    "| **Parameter**   | **Description**   | **Default**        | **Required**  |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|---------------|\n",
    "| **`geometry`**   | A 1-D array-like object (e.g., `numpy.array` or `geopandas.GeoSeries`) containing shapely geometries such as Polygons or LineStrings.| - | Yes|\n",
    "| **`x_coords`**   | Name of the coordinate containing `x` values (e.g., longitude or the first vertex coordinate of the geometry).| -  | Yes  |\n",
    "| **`y_coords`**   | Name of the coordinate containing `y` values (e.g., latitude or the second vertex coordinate of the geometry).                                                                                                                                                 | -                  | Yes           |\n",
    "| **`stats`**      | Spatial aggregation method. Options include standard methods like `\"mean\"`, `\"min\"`, `\"max\"`, `\"quantile(q=0.2)\"`, custom `Callable` functions, or sequences in `(name, func, {kwargs})` format.                                                               | `\"mean\"`           | No            |\n",
    "| **`name`**       | Name of the dimension that will hold the geometry.                                                                                                                                                                                                            | `\"geometry\"`       | No            |\n",
    "| **`index`**      | Determines whether the `GeoSeries` index is attached to the geometry dimension. Options: `True` (attach index), `False` (do not store index).                                                                                                                 | `None`             | No            |\n",
    "| **`method`**     | Data extraction method: `\"rasterize\"` (faster, may lose detail), `\"iterate\"` (uses `geometry_mask`), `\"exactextract\"` (precise stats using raster cell coverage).                                                                                              | `\"rasterize\"`      | No            |\n",
    "| **`all_touched`** | If `True`, considers all pixels touched by geometries. If `False`, includes only pixels whose centers fall within the geometry. Applies to `\"rasterize\"` and `\"iterate\"` methods.                                                                           | `False`            | No            |\n",
    "| **`n_jobs`**     | Number of parallel threads for computation. Set to `-1` to use all available CPU cores. Applies only to the `\"iterate\"` method.                                                                                                                               | -                  | No            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68647e-b396-42d6-ab63-7e99f782e8f1",
   "metadata": {},
   "source": [
    "Try the `iterate` method, which allows us to specify the number of parallel threads for computation. Note that, as indicated in the parameters table, parallelization is only supported for the iterate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83327a0c-e289-4b88-a413-cab4285ea6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aggregated_par = ds.xvec.zonal_stats(\n",
    "    shp.geometry,      \n",
    "    x_coords=\"x\",       \n",
    "    y_coords=\"y\", \n",
    "    stats=\"mean\",\n",
    "    name=\"basin_geometry\",    \n",
    "    index=True,\n",
    "    method=\"iterate\",\n",
    "    all_touched=True,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f074a-7d28-4f53-bbd5-f70c673d2c24",
   "metadata": {},
   "source": [
    "This original dataset is indexed by longitude and latitude representing the spatial grid. When aggregating using `ds.xvec.zonal_stats`, we are replacing these two dimensions with a single one with shapely geometry. Below is a preview of the dataset with the aligned vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debc709-7c80-4515-a428-50c9ea5567a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e9164-93e9-43eb-8e38-7841c333c92c",
   "metadata": {},
   "source": [
    "The current dataset includes vector data, allowing us to easily subset it for a specific subbasin using `aggregated_par.isel(geometry=0)`. However, we don’t know which subbasin this corresponds to. Adding basin names to the dataset makes it significantly easier to subset data based on basin names for various applications. This can be achieved using `assign_coords`, which requires: (1) a name for the new coordinate (`basin_name`), (2) the dimension to associate it with (`basin_geometry`), and (3) the values for the new coordinate (provided as `shp.name.values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a85778-8065-4258-b957-90207cd968f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_par = (\n",
    "    aggregated_par\n",
    "    .assign_coords(basin_name=(\"basin_geometry\", shp.name.values))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ea753-487f-42d0-ad5e-36b2fb9600f4",
   "metadata": {},
   "source": [
    "Preview the dataset with added basin names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c725b67-ce71-45c7-b907-d6ba2465539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda009a4-7358-4f7e-af42-c2d2f009ab64",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0f7fa; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Subset Example:</strong> Plot the spatially averaged precipitation from the AORC data for January 2019 in the Jordan River subbasin within the Great Salt Lake Basin. Note that the area of this subbasin is approximately 9,258 square kilometers (calculated using <code>shp.geometry.area / 1e6</code>), and since the precipitation data are at an hourly time step, this task may take some time to complete.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2277595-0a73-46de-a961-bb12b77150cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dat = (aggregated_par\n",
    "       .where(aggregated_par.basin_name == \"Jordan\", drop=True)\n",
    "       .sel(time=slice('2019-01-01', '2019-02-01'))\n",
    "       .RAINRATE.compute() # triggers the computation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4eaab-c9ec-49fc-a078-178a28b05727",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "dat.plot(ax=ax)\n",
    "ax.set_title(f'Spatially Averaged Precipitation for Jordan River Subbasin')\n",
    "ax.set_xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd85496-ed41-4c97-9ef1-b37c82f6d706",
   "metadata": {},
   "source": [
    "### 6. Other aggregation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffadd3-6b90-4ed7-b424-778aeb171754",
   "metadata": {},
   "source": [
    "We can pass a list of aggregation methods, which will create an additional dimension in the resulting dataset. This can be done by using:\n",
    "\n",
    "> \"... a string representing an aggregation method available as DataArray/Dataset or *GroupBy methods like DataArray.mean, DataArray.min or DataArray.max. Alternatively, you can pass a callable accepted by DataArray/Dataset.reduce. Alternatively, you can pass a tuple in a format (name, func) where name is used as a coordinate and func is either known string as above or a callable, or (name, func, {kwargs}), if you need to pass additional keyword arguments [[Xvec documentation](https://xvec.readthedocs.io/en/latest/zonal_stats.html)].\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f43a91-cc44-47ad-85fb-325e9e7a05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "aggregated_custom = ds.xvec.zonal_stats(\n",
    "    shp.geometry,\n",
    "    x_coords=\"x\",\n",
    "    y_coords=\"y\",\n",
    "    stats=[\n",
    "        \"mean\",\n",
    "        \"sum\",\n",
    "        # (\"quantile\", \"quantile\", dict(q=[0.1, 0.2, 0.3])),  # commented out this as  the \"quantile\" aggregation method is not compatible with the current chunking of the dataset along the y dimension. We may need to combine all chunks along the `y` using ds.chunk({\"y\": -1. \n",
    "        (\"numpymean\", numpy.nanmean),\n",
    "        numpy.nanstd,\n",
    "    ],\n",
    "    name=\"basin_geometry\",    \n",
    "    index=True,\n",
    "    method=\"iterate\",\n",
    "    all_touched=True,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49feef49-1930-4a52-b4f0-1bfbc5e745bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_custom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
