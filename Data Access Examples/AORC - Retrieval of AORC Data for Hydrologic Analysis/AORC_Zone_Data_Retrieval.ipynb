{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db250eac-dd32-4895-88bb-72ccbb8f5ca2",
   "metadata": {},
   "source": [
    "# Retrieving and Aggregating AORC Data for a Spatial Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1958827-496d-44a2-9b2d-ba557c8fb0fa",
   "metadata": {},
   "source": [
    "**Authors:** \n",
    "\n",
    "<ul style=\"line-height:1.5;\">\n",
    "<li>Ayman Nassar <a href=\"mailto:ayman.nassar@usu.edu\">(ayman.nassar@usu.edu)</a></li>\n",
    "<li>Pabitra Dash <a href=\"mailto:pabitra.dash@usu.edu\">(pabitra.dash@usu.edu)</a></li>\n",
    "<li>Homa Salehabadi <a href=\"mailto:homa.salehabadi@usu.edu\">(homa.salehabadi@usu.edu)</a></li>\n",
    "<li>David Tarboton <a href=\"mailto:david.tarboton@usu.edu\">(david.tarboton@usu.edu)</a></li>\n",
    "<li>Anthony Castronova <a href=\"acastronova@cuahsi.org\">(acastronova@cuahsi.org)</a></li>\n",
    "</ul>\n",
    "\n",
    "**Last Updated:** 1/20/25\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This notebook provides code examples for retrieving NOAA Analysis of Record for Calibration (AORC) data from Amazon Web Services (AWS). It is intended to make it easy for researchers to access data for a specific spatial domain specified by a shapefile. It also allows for data aggregation at different time scales than what NOAA originally provided.\n",
    "\n",
    "**Audience:**\n",
    "\n",
    "Researchers who are familiar with Jupyter Notebooks, basic Python and basic hydrologic data analysis.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook takes as inputs a shapefile of a specific location, start and end dates for the desired period, a variable name, and a preferred time aggregation interval. It then retrieves data from AWS, aggregates it over the specified time interval, displays the data as a plot, and saves it as a CSV file. The present implementation aggregates over the bounding box defined by the shapefile, and as such can take any shapefile as input. Future versions will aggregate over individual shapes within the input shapefile.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This notebook uses AORC data developed and published by NOAA on Amazon Web Services (AWS) as described in detail in this registry of open data entry <https://registry.opendata.aws/noaa-nws-aorc/>. The AORC dataset is a gridded record of near-surface weather conditions covering the continental United States and Alaska and their hydrologically contributing areas. It is defined on a latitude/longitude spatial grid with a mesh length of 30 arc seconds (~800 m), and a temporal resolution of one hour. This notebook uses the Zarr format files of version 1.1 of the AORC data. Zarr is a format for storage of chunked, compressed, N-dimensional arrays, designed to support storage using distributed systems such as cloud object stores (<https://zarr.dev/>).\n",
    "\n",
    "\n",
    "**Software Requirements:**\n",
    "\n",
    "This notebook has been tested on the CIROH Jupyterhub, CyberGIS Jupyter for Water and CUAHSI JupyterHub deployments. It relies on the general-purpose Jupyter computing environment, which has the following specific Python libraries and code used in this notebook: \n",
    "\n",
    " > numpy: 1.26.4     \n",
    "   geopandas: 0.14.3  \n",
    "   pandas: 2.2.1  \n",
    "   matplotlib: 3.8.3   \n",
    "   contextily: 1.6.2    \n",
    "   shapely: 2.0.3\n",
    "\n",
    "It also uses code from aorc_utils.py that accompanies this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008462b4-5ebb-4b85-b513-082c52e64f68",
   "metadata": {},
   "source": [
    "### 1. Import Python Libraries Needed to Run this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193db734-19c9-4a1c-a291-fa43d2b94e85",
   "metadata": {},
   "source": [
    "The `contextily` library is used in this notebook for mapping. It may not be installed in your Python environment by default so should be installed before you work with it. Use the following command to install the contextily library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979f4f1-5316-47aa-818e-929bf1bfd911",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ee846-cf21-44ca-a486-db43574c7f9e",
   "metadata": {},
   "source": [
    "Import the libraries needed to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d61a5-bf09-4b2d-930f-b1bf944fa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx  # For adding a basemap\n",
    "from shapely.geometry import box\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from aorc_utils import get_conus_bucket_url, load_dataset, reproject_coordinates, get_variable_code, get_aggregation_code, get_time_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63434088-5ea0-4eef-a2f0-3521b0e49cad",
   "metadata": {},
   "source": [
    "### 2. Set Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0857572-0a64-496a-8d57-b585ecc5417d",
   "metadata": {},
   "source": [
    "Use the cells in this section of the notebook to set the input values that specify the data to retrieve.  Note that a shapefile holding Great Salt Lake subbasins has been provided with this resource. Add your own shapefile and provide the path to its location to retrieve data for a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc77e5-70cd-483e-807e-dfbab644c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start date - In Year-Month-Day format the earliest start date can be '1979-02-01'\n",
    "start_datetime = '2020-01-01'\n",
    "\n",
    "# End date - In Year-Month-Day format the latest end date can be '2023-01-31'\n",
    "end_datetime = '2020-12-31'\n",
    "\n",
    "# File path to the shape file to used as an input\n",
    "shapefile_path = \"./GSLSubbasins/GSLSubbasins.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca6caa-d9b4-4584-9794-a241c5c7e73a",
   "metadata": {},
   "source": [
    "**The followings are valid variables to retrieve data:**\n",
    "\n",
    "- 'Total Precipitation' (APCP_surface): Hourly total precipitation (kgm-2 or mm)\n",
    "- 'Air Temperature' (TMP_2maboveground): Temperature (at 2 m above-ground-level (AGL)) (K)\n",
    "- 'Specific Humidity' (SPFH_2maboveground): Specific humidity (at 2 m AGL) (g g-1)\n",
    "- 'Downward Long-Wave Radiation Flux' (DLWRF_surface): longwave (infrared) radiation flux (at the surface) (W m-2)\n",
    "- 'Downward Short-Wave Radiation Flux' (DSWRF_surface): Downward shortwave (solar) radiation flux (at the surface) (W m-2)\n",
    "- 'Pressure (PRES_surface): Air pressure (at the surface) (Pa)\n",
    "- 'U-Component of Wind' (UGRD_10maboveground): (west to east) - components of the wind (at 10 m AGL) (m s-1)\n",
    "- 'V-Component of Wind' (VGRD_10maboveground): (south to north) - components of the wind (at 10 m AGL) (m s-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376ba66-511e-440b-b2cc-a68661c70126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined inputs - this can be any variable name as listed above\n",
    "variable_name = 'Total Precipitation'\n",
    "\n",
    "# User-defined aggregation interval - valid values are 'hour','day','month','year'\n",
    "agg_interval = 'month'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e0440-0d84-4733-b6ee-3fd42834f3af",
   "metadata": {},
   "source": [
    "### 3. Display the Map and Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2250e3-bd61-4eec-a4dd-8230bf432b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Get the total bounds of the shapefile and display it\n",
    "bounds = gdf.total_bounds\n",
    "\n",
    "# Create a bounding box from the bounds\n",
    "bbox = box(bounds[0], bounds[1], bounds[2], bounds[3])\n",
    "\n",
    "# Convert the bounding box into a GeoDataFrame\n",
    "bbox_gdf = gpd.GeoDataFrame([bbox], columns=['geometry'], crs=gdf.crs)\n",
    "\n",
    "# Create a layout for the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Display the shapefile\n",
    "gdf.plot( ax=ax, color='none', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Plot the bounding box on the same plot (using the ax object from the previous plot)\n",
    "bbox_gdf.boundary.plot(ax=ax, color=\"red\", linestyle='--', linewidth=2)\n",
    "\n",
    "# Add a topographic basemap using contextily \n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs=gdf.crs.to_string(), alpha=0.8)\n",
    "\n",
    "# Add title\n",
    "ax.set_title(\"Great Salt Lake Basin and Bounding Box\", fontsize=12)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Customize x and y axis labels\n",
    "ax.set_xlabel(\"Longitude\", fontsize=12)\n",
    "ax.set_ylabel(\"Latitude\", fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c871-2a7f-4bd0-83c5-15f64f6478d9",
   "metadata": {},
   "source": [
    "### 4. Virtually Load the Data Array \n",
    "This block of code maps the input variable and aggregation interval onto the variable encoding used in the Zarr bucket storage.  It then loads the virtual xarray dataset for the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fa72e-ff52-4f51-bdb3-fa198cd25e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variable code\n",
    "variable_code = get_variable_code(variable_name)\n",
    "\n",
    "# Get the S3 bucket data file URL\n",
    "url = get_conus_bucket_url(variable_code)\n",
    "ds = load_dataset(url)\n",
    "\n",
    "# Print the dataset (ds) of selected variable\n",
    "print(ds)\n",
    "\n",
    "# Print the unit of the slected variable in AORC dataset\n",
    "print(f\"The unit of {list(ds.data_vars)[0]} is {ds[list(ds.data_vars)[0]].attrs.get('units', 'No units specified')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48d70d-462b-405a-a2ef-eeb1e08cfc7c",
   "metadata": {},
   "source": [
    "### 4. Subset and Aggregate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c2a68-f9b8-4155-af0c-fc3c71a9acb6",
   "metadata": {},
   "source": [
    "The output is a dataframe with datetime in the first column, followed by values for the specified variable.\n",
    "\n",
    "This block of code first projects the bounding box coordinates to the coordinate system used by the AORC data. The input coordinate system is from the geopandas data frame from the shapefile. The AORC data coordinate system is a Lambert Conformal Conic projection used by the National Water Model. Curious users could examine ds.crs.esri_pe_string to see details. \n",
    "\n",
    "Xarray functions are used to aggregate both in space and time. Xarray defaults are used, though other xarray options could be set if needed.  The time step of the input AORC data is hourly and spacing of the AORC data 1 km.\n",
    "\n",
    "The results is saved in a data frame ds_subset.df which holds as columns time (date) and the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29644913-6a0f-483d-af3d-58678405d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The above code line is for measuring code execution time for this cell\n",
    "\n",
    "# Reproject coordinates\n",
    "x_min, y_min = reproject_coordinates(ds, bounds[0], bounds[1], gdf.crs)\n",
    "x_max, y_max = reproject_coordinates(ds, bounds[2], bounds[3], gdf.crs)\n",
    "\n",
    "# Get the aggregation code from the aggregation interval\n",
    "agg_code = get_aggregation_code(agg_interval)\n",
    "\n",
    "# Subsetting and aggregating a user-defined variable\n",
    "variable_code_cap = variable_code.upper()\n",
    "if variable_code == 'precip':\n",
    "    ds_subset = ds.sel(y=slice(y_min, y_max), x=slice(x_min, x_max)).loc[dict(time=slice(start_datetime,end_datetime))]\n",
    "    ds_subset_df = ds_subset['RAINRATE'].resample(time=agg_code).sum().mean(['x', 'y']).to_dataframe()*3600\n",
    "else:\n",
    "    ds_subset = ds.sel(y=slice(y_min, y_max), x=slice(x_min, x_max)).loc[dict(time=slice(start_datetime,end_datetime))]\n",
    "    ds_subset_df = ds_subset[variable_code_cap].resample(time=agg_code).mean().mean(['x', 'y']).to_dataframe()\n",
    "\n",
    "print(ds_subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802d1f5-cd5d-4f27-b5bd-d5068dc633a2",
   "metadata": {},
   "source": [
    "### 5. Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6303100-206c-4a96-8749-5f9263088c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the time index and column to plot\n",
    "time_list=pd.to_datetime(ds_subset_df.index)\n",
    "data_list = ds_subset_df.iloc[:,0]\n",
    "    \n",
    "# Setup the Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_list, data_list, color='blue', linewidth=2, marker='o', markersize=6, markerfacecolor='red', markeredgewidth=1)\n",
    "plt.title(f'{variable_name} ({start_datetime[:]} - {end_datetime[:]})', fontsize=16)\n",
    "plt.xlabel(f'{agg_interval.capitalize()}', fontsize=14)\n",
    "plt.ylabel(f'{variable_name} (mm/{agg_interval})', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Adding annotations or additional information if needed\n",
    "# Example: adding a trend line\n",
    "z = np.polyfit(range(len(time_list)), data_list, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(time_list, p(range(len(time_list))), color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Saving the plot\n",
    "plt.savefig(f'{agg_interval}_{variable_code}_plot_for_area.png', dpi=800)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3c688-f6b8-4a01-8486-95430322102e",
   "metadata": {},
   "source": [
    "### 6. Save the Data as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cc1fc-ffb8-4599-887c-03809536959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "file_path = f\"{variable_name}_over_box.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "ds_subset_df.to_csv(file_path, index=True)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
