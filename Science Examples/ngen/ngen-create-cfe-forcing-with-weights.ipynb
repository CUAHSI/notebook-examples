{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f45d1d3-2ce5-487c-8d89-a2e0ec3870ec",
   "metadata": {},
   "source": [
    "# Prepare Basin Averaged Forcing for NGEN using Pre-generated Weights\n",
    "\n",
    "**Authors:**  \n",
    "   - Tony Castronova <acastronova@cuahsi.org>    \n",
    "   - Irene Garousi-Nejad <igarousi@cuahsi.org>  \n",
    "    \n",
    "**Last Updated:** \n",
    "\n",
    "**Description**:  \n",
    "\n",
    "<!-- The purpose of this Jupyter Notebook is to demonstrate how to prepare basin averaged forcing input from for the [NOAA Next Generation (NextGen) Water Resource Modeling Framework](https://github.com/NOAA-OWP/ngen). This notebook demonstrates how these data can be prepared from AORC v1.1 however a similar process can be applied to other datasets.\n",
    "\n",
    "**Software Requirements**:  \n",
    "\n",
    "The software and operating system versions used to develop this notebook are listed below. To avoid encountering issues related to version conflicts among Python packages, we recommend creating a new environment variable and installing the required packages specifically for this notebook.\n",
    "\n",
    "Tested on: MacOS Ventura 13.2.1 (`python: 3.10.8`) \n",
    "\n",
    "> dask: 2022.12.1  \n",
    "  distributed: 2022.12.1  \n",
    "  geopandas: 0.12.2   \n",
    "  numpy: 1.23.5   \n",
    "  xarray: 2022.12.0  \n",
    "  pyproj: 3.4.1             \n",
    "  pandas: 1.5.2  \n",
    "  requests: 2.28.1  \n",
    "  rioxarray: 0.13.3  \n",
    "  geocube: 0.3.3   \n",
    " -->\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c353a2-226b-4dfb-8db4-c2fbcadb2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import dask\n",
    "import numpy\n",
    "import xarray\n",
    "import pyproj\n",
    "import pandas\n",
    "import requests\n",
    "import geopandas\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede0541-b85e-4fc8-8e2d-d2635f281d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install rioxarray and geocube\n",
    "!pip install rioxarray -q\n",
    "!pip install geocube -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4450f-592c-4248-95a3-a3d3a2aedfc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "from geocube.api.core import make_geocube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79279a53-22ae-40cc-a05c-67188f18ccc4",
   "metadata": {},
   "source": [
    "Initiate the Dask client. This will enable us to parallelize our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005eb820-57c2-4ad6-81e8-26ed37d4f075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(client.dashboard_link)\n",
    "except:    \n",
    "    # The client should be customized to your workstation resources.\n",
    "    # This is configured for a \"Large\" instance on ciroh.awi.2i2c.cloud\n",
    "    # client = Client()\n",
    "    client = Client(n_workers=8, memory_limit='10GB') # Large Machine\n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dc74c-01a2-4fef-bdfe-f021de978be4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bb9cd-cf11-4089-accb-fd50b1a99230",
   "metadata": {},
   "source": [
    "## Load Forcing Data into Memory\n",
    "\n",
    "In this notebook we'll be working with AORC v1.1 meteorological forcing. These data are publicly available for the `HUC 16` region from 01/01/2010 - 12/31/2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42025c18-adc1-46b3-b06d-beb030eb8c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the selected watershed boundary \n",
    "wb_id = 'wb-2917533_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ee1ef-6440-4fe3-8cbd-7c04bbdc1070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the paths that will be used to load the data.\n",
    "catalog_base_url = 'https://thredds.hydroshare.org/thredds/catalog'\n",
    "dods_base_url = 'https://thredds.hydroshare.org/thredds/dodsC'\n",
    "url = f'{catalog_base_url}/aorc/data/v1.1/16/catalog.xml'\n",
    "root = ET.fromstring(requests.get(url).text)\n",
    "ns = '{http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6237c0-d800-48b4-ab38-580e1c4c9ed2",
   "metadata": {},
   "source": [
    "Locate the datasets that are available by querying the Thredds Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899d335-e7b6-48c3-b029-13ee41fbde93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use xpath top select all \"dataset\" elements.\n",
    "elems = root.findall(f'.//{ns}dataset')\n",
    "\n",
    "# loop through results and extract the \"urlPath\" attribute values\n",
    "paths = []\n",
    "for elem in elems:\n",
    "    atts = elem.attrib\n",
    "    if 'urlPath' in atts.keys():\n",
    "        paths.append(f\"{dods_base_url}/{atts['urlPath']}\")\n",
    "        \n",
    "# use regex to isolate only files that end with \".nc\"\n",
    "paths = list(filter(re.compile(\"^.*\\.nc$\").match, paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e005f-0a63-41e7-b9e4-f6d21a3fcec8",
   "metadata": {},
   "source": [
    "Load the dataset using `xarray` and add spatial metadata to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ebd2d-86c4-4551-9f68-1441d5b108c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xarray.open_mfdataset(paths,\n",
    "                           concat_dim='time',\n",
    "                           combine='nested',\n",
    "                           parallel=True,\n",
    "                           chunks={'time': 10, 'x': 285, 'y':275},\n",
    "                           engine='netcdf4'\n",
    "                          )\n",
    "\n",
    "ds_meta = xarray.open_dataset('http://thredds.hydroshare.org/thredds/dodsC/hydroshare/resources/2a8a3566e1c84b8eb3871f30841a3855/data/contents/WRF_Hydro_NWM_geospatial_data_template_land_GIS.nc')\n",
    "\n",
    "leny = len(ds_meta.y)\n",
    "x = ds_meta.x[373 : 1227 + 1].values\n",
    "y = ds_meta.y[leny - 2405 - 1 : leny - 1586].values\n",
    "\n",
    "X, Y = numpy.meshgrid(x, y)\n",
    "\n",
    "# define the input crs\n",
    "wrf_proj = pyproj.Proj(proj='lcc',\n",
    "                       lat_1=30.,\n",
    "                       lat_2=60., \n",
    "                       lat_0=40.0000076293945, lon_0=-97., # Center point\n",
    "                       a=6370000, b=6370000) \n",
    "\n",
    "# define the output crs\n",
    "wgs_proj = pyproj.Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "# transform X, Y into Lat, Lon\n",
    "transformer = pyproj.Transformer.from_crs(wrf_proj.crs, wgs_proj.crs)\n",
    "lon, lat = transformer.transform(X, Y)\n",
    "\n",
    "ds = ds.assign_coords(lon = (['y', 'x'], lon))\n",
    "ds = ds.assign_coords(lat = (['y', 'x'], lat))\n",
    "ds = ds.assign_coords(x = x)\n",
    "ds = ds.assign_coords(y = y)\n",
    "\n",
    "ds.x.attrs['axis'] = 'X'\n",
    "ds.x.attrs['standard_name'] = 'projection_x_coordinate'\n",
    "ds.x.attrs['long_name'] = 'x-coordinate in projected coordinate system'\n",
    "ds.x.attrs['resolution'] = 1000.  # cell size\n",
    "\n",
    "ds.y.attrs['axis'] = 'Y' \n",
    "ds.y.attrs['standard_name'] = 'projection_y_coordinate'\n",
    "ds.y.attrs['long_name'] = 'y-coordinate in projected coordinate system'\n",
    "ds.y.attrs['resolution'] = 1000.  # cell size\n",
    "\n",
    "ds.lon.attrs['units'] = 'degrees_east'\n",
    "ds.lon.attrs['standard_name'] = 'longitude' \n",
    "ds.lon.attrs['long_name'] = 'longitude'\n",
    "\n",
    "ds.lat.attrs['units'] = 'degrees_north'\n",
    "ds.lat.attrs['standard_name'] = 'latitude' \n",
    "ds.lat.attrs['long_name'] = 'latitude'\n",
    "\n",
    "# add crs to netcdf file\n",
    "ds.rio.write_crs(ds_meta.crs.attrs['spatial_ref'], inplace=True\n",
    "                ).rio.set_spatial_dims(x_dim=\"x\",\n",
    "                                            y_dim=\"y\",\n",
    "                                            inplace=True,\n",
    "                                           ).rio.write_coordinate_system(inplace=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815a1c7-15d2-41e8-bf7b-349809ac9c8c",
   "metadata": {},
   "source": [
    "## Add spatial reference to the model domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff03fee-5f8b-40ab-a472-382416f5f8de",
   "metadata": {},
   "source": [
    "Load the geodatabase of our `ngen` domain. This can be obtained using the `ngen-hydrofabric-subset.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885ec72-55e8-4cd2-a0ab-a83071d23696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare geometries for spatial averaging\n",
    "gdf = geopandas.read_file(f'{wb_id}/config/{wb_id.split(\"_\")[0]}_upstream_subset.gpkg', layer='divides')\n",
    "\n",
    "# convert these data into the projection of our forcing data\n",
    "target_crs = pyproj.Proj(proj='lcc',\n",
    "                       lat_1=30.,\n",
    "                       lat_2=60., \n",
    "                       lat_0=40.0000076293945, lon_0=-97., # Center point\n",
    "                       a=6370000, b=6370000) \n",
    "\n",
    "gdf = gdf.to_crs(target_crs.crs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c042ce2-f0d9-42be-8e16-82f280a77add",
   "metadata": {},
   "source": [
    "## Clip AORC to the extent of the subset hydrofabric geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23683e7a-eff0-43cc-ab87-a470ad60cb5b",
   "metadata": {},
   "source": [
    "Add catchment ids to the geodataset. These will be used to perform zonal statistics later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a7efb-fd65-4bf2-8021-2e411a1c7ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create zonal id column\n",
    "gdf['cat'] = gdf.id.str.split('-').str[-1].astype(int)\n",
    "\n",
    "# clip AORC to the extent of the hydrofabric geometries\n",
    "ds = ds.rio.clip(gdf.geometry.values,\n",
    "                 gdf.crs,\n",
    "                 drop=True,\n",
    "                 invert=False)\n",
    "\n",
    "# select a single array of data to use as a template\n",
    "lwdown_data = ds.isel(time=0).LWDOWN\n",
    "\n",
    "# create a grid for the geocube\n",
    "out_grid = make_geocube(\n",
    "    vector_data=gdf,\n",
    "    measurements=[\"cat\"],\n",
    "    like=ds # ensure the data are on the same grid\n",
    ")\n",
    "\n",
    "# add the catchment variable to the original dataset\n",
    "ds = ds.assign_coords(cat = (['y','x'], out_grid.cat.data))\n",
    "\n",
    "# compute the unique catchment IDs which will be used to compute zonal statistics\n",
    "catchment_ids = numpy.unique(ds.cat.data[~numpy.isnan(ds.cat.data)])\n",
    "\n",
    "print(f'The dataset contains {len(catchment_ids)} catchments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bb7c3-2560-4b60-8fad-b7b6dea00dbb",
   "metadata": {},
   "source": [
    "## Preview the gridded catchments over the watershed vector boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc640d57-4735-4fb7-9459-e9b997413f65",
   "metadata": {},
   "source": [
    "Note that the method we're using will associate grid cell with the watershed that it overlaps the most with. There are more advanced ways to create a mapping using various interpolation methods that will distribute values cells across all watershed boundaries that they intersect with. This is left as a future exercize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea60035-4514-4c13-9f35-b4936bd27405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "\n",
    "# plot the gridded catchment mapping\n",
    "ds.cat.plot()\n",
    "\n",
    "## create a discrete color mapping such that each catchment \n",
    "## is represented by a single color\n",
    "# cmap = colors.ListedColormap(['green', 'lightskyblue', 'cyan', 'red', 'navy'])\n",
    "# bounds = [catchment_ids[0]] + [c+0.9 for c in catchment_ids]\n",
    "# norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "# ds.cat.plot(cmap=cmap, norm=norm, ax=ax)\n",
    "\n",
    "# preview map geometries\n",
    "gdf.iloc[:].plot(ax=ax, linewidth=2, edgecolor='k', facecolor='None');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63e278-0a9e-4256-aaab-dd64bc4e1eaf",
   "metadata": {},
   "source": [
    "## Compute basin-averaged forcing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcaeb1-1cfa-4384-891f-af04e6adfe07",
   "metadata": {},
   "source": [
    "Define functions that will be used to perform basin averages on the AORC data. These functions leverage `dask` to parallelize the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288b205-9592-42c5-9cf4-af72389485ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call once per catchment\n",
    "# distribute zonal stats to sub processes\n",
    "def perform_zonal_computation(ds, cat_id):\n",
    "\n",
    "    # subset by catchment id\n",
    "    ds_catchment = ds.where(ds.cat==cat_id, drop=True)\n",
    "#    ds_catchement_future = client.scatter(ds_catchment, broadcast=True)\n",
    "    \n",
    "    delayed = []\n",
    "    # loop over variables\n",
    "    for variable in ['LQFRAC', 'LWDOWN', 'PSFC',\n",
    "                     'Q2D', 'RAINRATE', 'SWDOWN',\n",
    "                     'T2D', 'U2D', 'V2D']:\n",
    "                \n",
    "        delay = dask.delayed(compute_zonal_mean)(ds_catchment[variable], variable)\n",
    "        delayed.append(delay)\n",
    "        \n",
    "    res = dask.compute(*delayed)\n",
    "    \n",
    "    # combine outputs (list of dicts) into a single dict.\n",
    "    res = {k: v for d in res for k, v in d.items()}\n",
    "    \n",
    "    # return results\n",
    "    return {f'cat-{int(cat_id)}': res}\n",
    "\n",
    "def compute_zonal_mean(ds, variable):   \n",
    "    return {variable: ds.mean(dim=['x','y']).values}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b37b00-c7a2-4bb9-9373-812e43efd609",
   "metadata": {},
   "source": [
    "Slice the data to the temporal period of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8e149-1eb2-43a5-9888-23405975c879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the start and end time of the data we want to use\n",
    "start_time = '2010-01-01 00:00'\n",
    "end_time = '2010-01-10 00:00'\n",
    "\n",
    "\n",
    "# isolate the desired time period of our data\n",
    "ds_subset = ds.sortby('time').sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f'The dataset contains {len(ds_subset.time)} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c369b39-3fef-4c1d-b3bd-92756ec56fdd",
   "metadata": {},
   "source": [
    "Let's rechunk our data now that we have many fewer elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d524d-4cf7-447b-8734-495524b6048c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset = ds_subset.chunk(chunks={'time': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc148297-8733-4bff-b718-e164e004ae13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00fb10-5bf0-4b31-b109-82c5d43796bc",
   "metadata": {},
   "source": [
    "Drop all data that we don't need. The goal here is to make the dataset as small as possible before we start running computations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c6a7c-1ea0-4b00-b27b-5e4d39c315d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop unused coordinates\n",
    "ds_subset = ds_subset.drop(['crs','lat','lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf0058-9098-4047-bf38-dab95b5c97ab",
   "metadata": {},
   "source": [
    "Tell `dask` to perform the subsetting computations on the data now. That way when we process the zonal statistics, the entire dataset won't need to be moved around. This will save a considerable amount of processing in future steps time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672b33-fbde-4c62-ba6d-26ac501ef2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "ds_subset = ds_subset.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e025945-b0b2-417e-b82a-5def64a4d5d4",
   "metadata": {},
   "source": [
    "Scatter the dataset to the cluster so all workers will have access to it. This is good practice and especially necessary if working on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756507ed-0cef-4e04-a31b-a16fc11b3ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scattered_ds = client.scatter(ds_subset, broadcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9bbd0-1ba7-4b04-95b9-94b0ef3c9af4",
   "metadata": {},
   "source": [
    "Build a list of `delayed` tasks. This will not execute the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b9b62-e48c-420a-b0be-14498b0045dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "delayed = []\n",
    "\n",
    "# loop over each catchment in our domain\n",
    "# create delayed tasks to compute zonal mean\n",
    "for cat_id in catchment_ids:\n",
    "    delay = dask.delayed(perform_zonal_computation)(scattered_ds, cat_id)\n",
    "    delayed.append(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9bbb0-11ac-4372-a092-d2ff4839aa48",
   "metadata": {},
   "source": [
    "Invoke the computation using `dask.compute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08011b8c-01ce-460d-9dfc-26f2730c94f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# run the computation\n",
    "results = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debf776-5ede-4f18-8595-90073448a27a",
   "metadata": {},
   "source": [
    "Save the basin averaged meteorological data in the format expected by `ngen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83002ba7-0a54-41a9-a367-d4ea23b9247b",
   "metadata": {},
   "source": [
    "The summarized AORC variables need to be mapped to the `ngen` model that we'll be using. The following table illustrates the mapping.\n",
    "\n",
    "|AORC Variable Name|NGEN Variable Name|Description|\n",
    "|---|---|---|\n",
    "| LQFRAC   | ---                   | Fraction of precipitation that is liquid vs. frozen (%)\n",
    "| LWDOWN   | DLWRF_surface         | Surface downward long-wave radiation flux (W m-2) \n",
    "| PSFC     | PRES_surface          | Surface Pressure (Pa)\n",
    "| Q2D      | SPFH_2maboveground    | 2-m Specific Humidity (kg kg-1)\n",
    "| RAINRATE | ---                   | precipitation_flux (mm s^-1)\n",
    "| SWDOWN   | DSWRF_surface         | Surface downward short-wave radiation flux (W m-2)\n",
    "| T2D      | TMP_2maboveground     | 2-m Air Temperature (K)\n",
    "| U2D      | UGRD_10maboveground   | 10-m U-component of wind (m s-1)\n",
    "| V2D      | VGRD_10maboveground   | 10-m V-component of wind (m s-1)\n",
    "| ---      | APCP_surface          | Surface precipitation (kg/m^2)\n",
    "\n",
    "Note: our `ngen` model is expecting shortwave and longwave radiation at a height of 0 meters above ground whereas the AORC data has values are 2 meters above ground.\n",
    "\n",
    "References: [tshirt_c.h](https://github.com/NOAA-OWP/ngen/blob/f2725dfbb52f3af5083ce927e69733edbf059f57/models/tshirt/include/tshirt_c.h#L52), [sample forcing csv](https://github.com/NOAA-OWP/ngen/blob/master/data/forcing/cat-27_2015-12-01%2000_00_00_2015-12-30%2023_00_00.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f0451-a11b-483e-9fbf-0b8e2bb43f89",
   "metadata": {},
   "source": [
    "## Save data as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bbc92-2813-4ec2-9a08-d8ab031a2f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# compute the date range for our data using start and end times\n",
    "# that were used in the subsetting process.\n",
    "dates = pandas.date_range(start_time, end_time, freq=\"60min\")\n",
    "\n",
    "# save the zonal means for each catchment\n",
    "for dat in results:\n",
    "    for cat in dat:\n",
    "        df = pandas.DataFrame({k:list(v) for k,v in dat[cat].items()})\n",
    "        df.fillna(0., inplace=True)\n",
    "        \n",
    "        # convert rainrate from mm/s to kg/m2\n",
    "        # mm/s - mm/hr = df.RAINRATE * 3600\n",
    "        # since the timestep is one hour, this is effectively the total rain in mm.\n",
    "        # 1 mm of rainfall is equal to 1kg/m2 so our conversion is:\n",
    "        # NOTE: we should only be considering the fraction of liquid precip which can\n",
    "        #       be computed using LQFRAC. However LQFRAC is zero for our data which \n",
    "        #       does not seem correct, so we'll assume that all precip is liquid. This\n",
    "        #       is something that needs to be revisited.\n",
    "        df['APCP_surface'] = df.RAINRATE * 3600\n",
    "\n",
    "        # rename columns to match the variable names expected by the ngen t-shirt model\n",
    "        df.rename(columns={\n",
    "            'LWDOWN'   : 'DLWRF_surface',\n",
    "            'PSFC'     : 'PRES_surface',\n",
    "            'Q2D'      : 'SPFH_2maboveground',\n",
    "            'SWDOWN'   : 'DSWRF_surface',\n",
    "            'T2D'      : 'TMP_2maboveground',\n",
    "            'U2D'      : 'UGRD_10maboveground',\n",
    "            'V2D'      : 'VGRD_10maboveground',\n",
    "            'RAINRATE' : 'precip_rate',\n",
    "        },\n",
    "                  inplace=True)\n",
    "        \n",
    "        # drop LQFRAC because it's not needed\n",
    "        df.drop(columns=['LQFRAC'], inplace=True)\n",
    "        \n",
    "        # add the time index\n",
    "        df['time'] = dates\n",
    "        df.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "        # write to file\n",
    "        with open(f'{wb_id}/forcings/{cat}.csv', 'w') as f:\n",
    "            # Note: saving \"precip_rate\" because this column exists in the example \n",
    "            #       forcing files. It's not clear if this is being used or not.\n",
    "            df.to_csv(f,\n",
    "                      columns = ['APCP_surface',\n",
    "                                 'DLWRF_surface',\n",
    "                                 'DSWRF_surface',\n",
    "                                 'PRES_surface',\n",
    "                                 'SPFH_2maboveground',\n",
    "                                 'TMP_2maboveground',\n",
    "                                 'UGRD_10maboveground',\n",
    "                                 'VGRD_10maboveground',\n",
    "                                 'precip_rate'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c667ef-619c-414d-aa57-794a018fa60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of catchments. \n",
    "print(gdf.shape[0])\n",
    "print(len(results))\n",
    "\n",
    "# If these are not equal, run the following code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e26c0-7ba6-40ad-908e-ea6b5fcb0f2d",
   "metadata": {},
   "source": [
    "Here is an example showing why some catchments are missing in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d0e73-33d7-40d0-8a1e-090ece72c5a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./figures/missing_catchment_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b28f1-12a0-46d7-ad5e-ee1a9e923c5f",
   "metadata": {},
   "source": [
    "Create synthetic forcing data for the missing catchments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c14988-7af8-4d77-92d2-92a99c5f33a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "computed_catchments = [list(r.keys())[0] for r in results]\n",
    "for cat_id in gdf['cat'].values:\n",
    "    known_catchment = f'cat-{int(cat_id)}'\n",
    "    if known_catchment not in computed_catchments:\n",
    "        print(f'Creating Synthetic Forcing for {known_catchment}')\n",
    "        synthetic_df = pandas.DataFrame(0, index=df.index, columns=['APCP_surface',\n",
    "                                                                    'DLWRF_surface',\n",
    "                                                                    'PRES_surface',\n",
    "                                                                    'SPFH_2maboveground',\n",
    "                                                                    'DSWRF_surface',\n",
    "                                                                    'TMP_2maboveground',\n",
    "                                                                    'UGRD_10maboveground',\n",
    "                                                                    'VGRD_10maboveground',\n",
    "                                                                    'precip_rate'])\n",
    "        # write to file\n",
    "        with open(f'{wb_id}/forcings/{known_catchment}.csv', 'w') as f:\n",
    "            df.to_csv(f,\n",
    "                      columns = ['APCP_surface',\n",
    "                                 'DLWRF_surface',\n",
    "                                 'DSWRF_surface',\n",
    "                                 'PRES_surface',\n",
    "                                 'SPFH_2maboveground',\n",
    "                                 'TMP_2maboveground',\n",
    "                                 'UGRD_10maboveground',\n",
    "                                 'VGRD_10maboveground',\n",
    "                                 'precip_rate'])\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
