{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3872d5-fe42-49a0-aee9-823d58af0e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Subset NGEN HydroFabric on S3\n",
    "\n",
    "**Authors:**  \n",
    "   - Tony Castronova <acastronova@cuahsi.org>    \n",
    "   - Irene Garousi-Nejad <igarousi@cuahsi.org>  \n",
    "    \n",
    "**Last Updated:** 06.19.2024   \n",
    "\n",
    "**Description**:  \n",
    "\n",
    "The purpose of this Jupyter Notebook is to demonstrate the process of preparing inputs required to execute the [NOAA Next Generation (NextGen) Water Resource Modeling Framework](https://github.com/NOAA-OWP/ngen). These inputs consist of the following components: \n",
    "\n",
    "- Hydrologic and hydrodynamic graphs based on the National Hydrologic Geospatial (Hydrofabric) data which includes catchments, nexus, and flowlines. \n",
    "- Model domain parameters represented as configuration files.\n",
    "- Meteorological forcing data.\n",
    "\n",
    " The Hydrofabric data can be accessed publicly through the AWS catalog. In this notebook, we use the **pre-release** version of the dataset, which represents the most recent version available on the Amazon S3 Bucket at the time of developing this notebook (https://nextgen-hydrofabric.s3.amazonaws.com/index.html#pre-release/). The configuration files encompass model default parameters, formulations, input and output paths, simulation time step, initial conditions, and other relevant settings. This example demonstrates the retrieval of hydrofabric data, followed by the extraction of necessary infromation for creating the parameter configuration file. These files are created for running Conceptual Functional Equivalent (CFE) model and Simple Logical Tautology Handler (SLoTH) in the NGEN framework. To prepare forcing data, run the *ngen-hydrofabric-subset.ipynb* Jupyter Notebook.\n",
    "\n",
    "**Software Requirements**:  \n",
    "\n",
    "The software and operating system versions used to develop this notebook are listed below. To avoid encountering issues related to version conflicts among Python packages, we recommend creating a new environment variable and installing the required packages specifically for this notebook.\n",
    "\n",
    "\n",
    "> boto3: 1.26.76  \n",
    "  dask-core: 2023.4.0  \n",
    "  fiona: 1.9.3  \n",
    "  fsspec: 2023.4.0  \n",
    "  geopandas: 0.12.2   \n",
    "  ipyleaflet: 0.17.2  \n",
    "  ipywidgets: 7.7.5   \n",
    "  matplotlib: 3.7.1   \n",
    "  netcdf4: 1.6.3   \n",
    "  numpy: 1.24.2  \n",
    "  pandas: 2.0.0  \n",
    "  requests: 2.28.2  \n",
    "  s3fs: 2023.4.0  \n",
    "  scipy: 1.10.1  \n",
    "  xarray: 2023.4.1\n",
    "  \n",
    "**Supplementary Code**\n",
    "\n",
    "This notebook relies on the following external scripts:  \n",
    "- `subset.py` - A script originally written by Nels Frazier to subset the NGen Hydrofabric\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c354f815-4e05-4a57-86d1-213a1536d801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import fiona\n",
    "import pandas\n",
    "import subset\n",
    "import pyproj\n",
    "import datetime\n",
    "import geopandas\n",
    "import ipyleaflet\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from sidecar import Sidecar\n",
    "from requests import Request\n",
    "from ipywidgets import Layout\n",
    "\n",
    "# from helpers import SideCarMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efd6e0-a664-4f98-98ec-c96bf55e2944",
   "metadata": {},
   "source": [
    "## 1. Create a map and load the Hydrofabric VPU geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c672fc-9635-460b-bef0-64338759f036",
   "metadata": {},
   "source": [
    "The data that we'll be using this notebook are located inside Vector Processing Unit 16. This area of the Hydrofabric covers the Great Basin. See the [NextGen Hydrofabric](https://mikejohnson51.github.io/hyAggregate/) help pages for more information regarding these data. A geopackage file may consist of many layers. Use `Fiona` to view the vector layers that are included within the `nextgen_16` geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ce5c9f-055f-41c2-a6f7-8cfe98d6678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hydrolocations',\n",
       " 'nexus',\n",
       " 'flowpaths',\n",
       " 'lakes',\n",
       " 'divides',\n",
       " 'network',\n",
       " 'flowpath_attributes',\n",
       " 'layer_styles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the layers include in the nextgen geopackage for VPU 16.\n",
    "fiona.listlayers('sample-data/nextgen_16.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f4bb-6011-4d34-8d93-f1fa8611d9dd",
   "metadata": {},
   "source": [
    "Our workflow will require the ID of the outlet catchment, so let's load the 'divides' layer. This layer contains over 30,000 features so we will not attempt to display it in the notebook. For reference, these data cover the following area:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/vpu-16.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6b0c0-78fd-496b-93f4-094e618bc4a1",
   "metadata": {},
   "source": [
    "Load these data into `Geopandas` and convert into the coordinate reference we will be using in out leaflet map (EPSG: 4326)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb9a948-a81e-4e3b-8950-8b3d46a042c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.read_file('sample-data/nextgen_16.gpkg', layer='divides')\n",
    "gdf = gdf.to_crs(epsg='4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e17d3-3a14-4c52-96bb-486e11ca277b",
   "metadata": {},
   "source": [
    "Create an interactive map interface for us to select our area of interest. This map will contain USGS river gauges and NHD+ reach geometries to help us select our area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81bb045-6887-4ed6-a953-8d55aeb4f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SideCarMap():\n",
    "    def __init__(self, basemap=ipyleaflet.basemaps.OpenStreetMap.Mapnik, gdf=None, plot_gdf=False, name='Map'):\n",
    "        self.selected_id = None\n",
    "        self.selected_layer = None\n",
    "        self.map = None\n",
    "        self.basemap = basemap\n",
    "        self.gdf = gdf\n",
    "        self.plot_gdf = False\n",
    "        self.name = name\n",
    "\n",
    "    def display_map(self):\n",
    "        defaultLayout=Layout(width='960px', height='940px')\n",
    "\n",
    "        self.map = ipyleaflet.Map(\n",
    "        basemap=ipyleaflet.basemap_to_tiles(ipyleaflet.basemaps.OpenStreetMap.Mapnik, layout=defaultLayout),\n",
    "            center=(45.9163, -94.8593),\n",
    "            zoom=9,\n",
    "            scroll_wheel_zoom=True,\n",
    "            tap=False\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # add USGS Gages\n",
    "        self.map.add_layer(\n",
    "            ipyleaflet.WMSLayer(\n",
    "                url='http://arcgis.cuahsi.org/arcgis/services/NHD/usgs_gages/MapServer/WmsServer',\n",
    "                layers='0',\n",
    "                transparent=True,\n",
    "                format='image/png',\n",
    "                min_zoom=8,\n",
    "                max_zoom=18,\n",
    "                )\n",
    "        )\n",
    "        \n",
    "        # add NHD+ Reaches\n",
    "        self.map.add_layer(\n",
    "            ipyleaflet.WMSLayer(\n",
    "                url='https://hydro.nationalmap.gov/arcgis/services/nhd/MapServer/WMSServer',\n",
    "                layers='6',\n",
    "                transparent=True,\n",
    "                format='image/png',\n",
    "                min_zoom=8,\n",
    "                max_zoom=18,\n",
    "                )\n",
    "        )\n",
    "\n",
    "        # add features from geopandas if they are provided\n",
    "        if self.gdf is not None:\n",
    "\n",
    "            # update the map center point\n",
    "            polygon = box(*self.gdf.total_bounds)\n",
    "            approx_center = (polygon.centroid.y, polygon.centroid.x)\n",
    "            self.map.center = approx_center\n",
    "\n",
    "            # bind the map handler function\n",
    "            self.map.on_interaction(self.handle_map_interaction)\n",
    "\n",
    "            if self.plot_gdf:\n",
    "                print('Loading GDF Features...', end='')\n",
    "                st = time.time()\n",
    "                geo_data = ipyleaflet.GeoData(geo_dataframe = self.gdf,\n",
    "                       style={'color': 'blue', 'opacity':0.5, 'weight':1.9,}\n",
    "                      )\n",
    "                self.map.add(geo_data)\n",
    "\n",
    "                print(f'{time.time() - st:0.2f} sec')\n",
    "\n",
    "        sc = Sidecar(title=self.name)\n",
    "        with sc:\n",
    "            display(self.map)\n",
    "        \n",
    "    def handle_map_interaction(self, **kwargs):\n",
    "    \n",
    "        if kwargs.get('type') == 'click':\n",
    "            print(kwargs)\n",
    "            lat, lon = kwargs['coordinates']\n",
    "            print(f'{lat}, {lon}')\n",
    "            \n",
    "            # query the reach nearest this point\n",
    "            point = shapely.Point(lon, lat)\n",
    "\n",
    "            # buffer the selected point by a small degree. This\n",
    "            # is a hack for now and Buffer operations should only\n",
    "            # be applied in a projected coordinate system in the future.\n",
    "            print('buffering')\n",
    "            pt_buf = point.buffer(0.001) \n",
    "\n",
    "            try:\n",
    "                # remove the previously selected layers\n",
    "                if self.selected_layer is not None:\n",
    "                    self.map.remove(self.selected_layer)\n",
    "                # while len(self.map.layers) > 3:\n",
    "                #     self.map.remove(self.map.layers[-1]);\n",
    "                \n",
    "                # query the FIM reach that intersects with the point\n",
    "                print('intersecting...')\n",
    "                mask = self.gdf.intersects(pt_buf)\n",
    "                print(f'found {len(self.gdf.loc[mask])} reaches')\n",
    "                print('saving selected...')\n",
    "                self.selected(value=self.gdf.loc[mask].iloc[0])\n",
    "\n",
    "                # highlight this layer on the map\n",
    "                wlayer = ipyleaflet.WKTLayer(\n",
    "                    wkt_string=self.selected().geometry.wkt,\n",
    "                    style={'color': 'green', 'opacity':1, 'weight':2.,})\n",
    "                self.map.add(wlayer)\n",
    "                self.selected_layer = self.map.layers[-1]\n",
    "                \n",
    "            except Exception: \n",
    "                print('Could not find reach for selected area')\n",
    "\n",
    "    # getter/setter for the selected reach\n",
    "    def selected(self, value=None):\n",
    "        if value is None:\n",
    "            if self.selected_id is None:\n",
    "                print('No reach is selected.\\nUse the map interface to select a reach of interest')\n",
    "            else:\n",
    "                return self.selected_id\n",
    "        else:\n",
    "            self.selected_id = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429d779-3b7f-4921-b97b-7ad141699b12",
   "metadata": {},
   "source": [
    "Launch the interactive map interface for us to select our area of interest. We'll pass the `divides` geometries that were loaded above so we can query them when the map is clicked and retrieve their metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1da5a2a-a66a-4579-854b-6ddc1bafb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SideCarMap(gdf=gdf, name='VPU 16 Map')\n",
    "m.display_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8729c-fd1e-4206-83b0-1fc0d1fbe0c8",
   "metadata": {},
   "source": [
    "After selecting our outlet catchment we can access it's metadata in the notebook using the `m.selected()` command. We're interested in the `id` attribute of this feature. It should look something like: `wb-2853613`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ac5a477-93df-48d3-b3a6-f9b09d606665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "divide_id                                                      cat-2853639\n",
       "toid                                                           nex-2853640\n",
       "type                                                               network\n",
       "ds_id                                                                  NaN\n",
       "areasqkm                                                           10.9161\n",
       "id                                                              wb-2853639\n",
       "lengthkm                                                          5.111081\n",
       "tot_drainage_areasqkm                                              71.1414\n",
       "has_flowline                                                          True\n",
       "geometry                 POLYGON ((-111.74018480488945 40.5554322243749...\n",
       "Name: 790, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_catchment = m.selected()\n",
    "selected_catchment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3093d-7d14-41a5-87d8-2d345ef6c963",
   "metadata": {},
   "source": [
    "## 3. Subset hydrofabric data for the selected area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63fd4e-b755-4648-89db-12bf5f063614",
   "metadata": {},
   "source": [
    "The following code passes the `id` and `VPU` values of the geometries selected on the map (`selected_df`) to the hydrofabric subsetting script (`subset.py`). The subsetting algorithm implemented in the code adopts a reverse tracing technique called `subset_upstream`. It systematically identifies and selects all the upstream divides, catchments, nexuses, and flowlines starting from the most downstream nexus linked to the chosen geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1e2038-b3f0-4666-b9a3-4cc27e3f98e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wb-2853639'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_catchment.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28af8c32-011d-4981-a614-1d5f62de424c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing VPU 16, wb-2853639 \n",
      "s3://lynker-spatial/hydrofabric/v20.1/gpkg/nextgen_16.gpkg\n",
      "Building Graph Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona._env:File /vsimem/20b95e8af0014b79819023a3856ecebb has GPKG application_id, but non conformant file extension\n",
      "WARNING:fiona._env:File /vsimem/e9545e91dc3d446e93130049afd2b284 has GPKG application_id, but non conformant file extension\n",
      "WARNING:fiona._env:File /vsimem/85d88ac341d04a01ac0f4253ee0402bc has GPKG application_id, but non conformant file extension\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "s3://lynker-spatial/hydrofabric/v20.1/gpkg/nextgen_16_cfe_noahowp.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# build the hydrofabric_url\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# the complete dataset can be found at: s3://lynker-spatial\u001b[39;00m\n\u001b[1;32m     10\u001b[0m hydrofabric_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://lynker-spatial/hydrofabric/v20.1/gpkg/nextgen_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvpu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gpkg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43msubset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset_upstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrofabric_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# move these files into a subdir to keep things orderly\u001b[39;00m\n\u001b[1;32m     14\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/work/notebooks/ngen/subset.py:271\u001b[0m, in \u001b[0;36msubset_upstream\u001b[0;34m(hydrofabric, ids)\u001b[0m\n\u001b[1;32m    268\u001b[0m parquet_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnextgen_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvpu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cfe_noahowp.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    269\u001b[0m parquet_path \u001b[38;5;241m=\u001b[39m  (\u001b[38;5;28mstr\u001b[39m(Path(\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39mparquet_name)\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    270\u001b[0m                  replace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 271\u001b[0m model_attributes \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3FileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43manon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread_pandas()\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m    275\u001b[0m model_attributes \u001b[38;5;241m=\u001b[39m (model_attributes\n\u001b[1;32m    276\u001b[0m                     \u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivide_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m                     \u001b[38;5;241m.\u001b[39mloc[cat_ids])\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Unsure if hydrolocations and lakes are being subset correctly.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/work/notebooks/ngen/conda-env/lib/python3.12/site-packages/pyarrow/parquet/core.py:1354\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1351\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[1;32m   1352\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/notebooks/ngen/conda-env/lib/python3.12/site-packages/pyarrow/dataset.py:782\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    771\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    772\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    773\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/Documents/work/notebooks/ngen/conda-env/lib/python3.12/site-packages/pyarrow/dataset.py:465\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    463\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    468\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    469\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    470\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    471\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    473\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "File \u001b[0;32m~/Documents/work/notebooks/ngen/conda-env/lib/python3.12/site-packages/pyarrow/dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    439\u001b[0m     paths_or_selector \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: s3://lynker-spatial/hydrofabric/v20.1/gpkg/nextgen_16_cfe_noahowp.parquet"
     ]
    }
   ],
   "source": [
    "\n",
    "id = selected_catchment.id\n",
    "vpu = 16\n",
    "\n",
    "output_files = []\n",
    " \n",
    "print(50*'-'+f'\\nProcessing VPU {vpu}, {id} ')\n",
    "st = time.time()\n",
    "# build the hydrofabric_url\n",
    "# the complete dataset can be found at: s3://lynker-spatial\n",
    "hydrofabric_url = f's3://lynker-spatial/hydrofabric/v20.1/gpkg/nextgen_{vpu}.gpkg'\n",
    "subset.subset_upstream(hydrofabric_url, id)\n",
    "\n",
    "# move these files into a subdir to keep things orderly\n",
    "counter = 1\n",
    "outpath = ids[i]\n",
    "while os.path.exists(outpath):\n",
    "    outpath = ids[i] + \"_\" + str(counter)\n",
    "    counter += 1\n",
    "os.mkdir(outpath)\n",
    "for subdir in ['config', 'forcings', 'outputs']:\n",
    "    os.mkdir(os.path.join(outpath, subdir))\n",
    "\n",
    "for f in [f'{ids[i]}_upstream_subset.gpkg',\n",
    "          'catchments.geojson',\n",
    "          'crosswalk.json',\n",
    "          'flowpath_edge_list.json',\n",
    "          'flowpaths.geojson',\n",
    "          'nexus.geojson',\n",
    "          'cfe_noahowp_attributes.csv']:\n",
    "    os.rename(f, os.path.join(outpath, 'config', f))\n",
    "    \n",
    "# output_files.append(f'{ids[i]}_upstream_subset.gpkg')\n",
    "print(f'Output files located at: {outpath}')\n",
    "print(f'Completed in {time.time() - st} seconds\\n'+50*'-')    \n",
    "\n",
    "outdir = Path(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6410e7b-f225-406d-adb9-c7cefdc07171",
   "metadata": {},
   "source": [
    "## 4. Add the Subset Hydrofabric to the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1ec5-666f-4e65-ad69-8d470d918eaf",
   "metadata": {},
   "source": [
    "Execute the following code cell to add the subset catchments and rivers from the geopackage file  as visual overlays on the map. In this section, we begin by reading the shapefiles stored within the geopackage file. We then proceed to transform the projection system of these shapes into Web Mercator, the desired coordinate reference system for our map. Next, we create a WKTLayer for each catchment and river in the subset shapefiles. A WKTLayer enables us to represent the shape's geometry using a WKT string, which provides a concise description of its spatial properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042621b-5525-41cf-97ee-2c5bfe886921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the shapes\n",
    "catchments = geopandas.read_file(f'{outdir}/config/{selected_df.id[0]}_upstream_subset.gpkg',\n",
    "                          layer='divides')\n",
    "rivers = geopandas.read_file(f'{outdir}/config/{selected_df.id[0]}_upstream_subset.gpkg',\n",
    "                          layer='flowpaths')\n",
    "\n",
    "# define the target projection as EPSG:4269 - Web Mercator\n",
    "# this is the default crs for leaflet\n",
    "target_crs = pyproj.Proj('4269')\n",
    "\n",
    "# transform the shapefile into EPSG:4269\n",
    "catchments = catchments.to_crs(target_crs.crs)\n",
    "rivers = rivers.to_crs(target_crs.crs)\n",
    "\n",
    "# add the catchments to the map\n",
    "for idx, shape in catchments.iterrows():\n",
    "    wkt = ipyleaflet.WKTLayer(wkt_string=shape.geometry.wkt)\n",
    "    wkt.style = {'color': 'green'}\n",
    "    m.add_layer(wkt)\n",
    "    \n",
    "# add the rivers to the map\n",
    "for idx, shape in rivers.iterrows():\n",
    "    wkt = ipyleaflet.WKTLayer(wkt_string=shape.geometry.wkt)\n",
    "    wkt.style = {'color': 'blue'}\n",
    "    m.add_layer(wkt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
