{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f45d1d3-2ce5-487c-8d89-a2e0ec3870ec",
   "metadata": {},
   "source": [
    "# Collect and Manipulate AORC Meteorological Data\n",
    "\n",
    "**Authors:**  \n",
    "   - Tony Castronova <acastronova@cuahsi.org>    \n",
    "   - Irene Garousi-Nejad <igarousi@cuahsi.org>  \n",
    "    \n",
    "**Last Updated:** \n",
    "\n",
    "**Description**:  \n",
    "\n",
    "This notebooks explores methods for working with large cloud data stores using tools such as `xarray`, `dask`, and `geopandas`. This exploration is uses the Analysis of Record for Collaboration (AORC) meteorological dataset that is used by the NOAA National Water Model. This notebook provides examples for how to access, slice, and visualize a large cloud-hosted dataset as well as an approach for aligning these data with watershed vector boundaries.\n",
    "\n",
    "The data used in this notebook can be found at https://registry.opendata.aws/nwm-archive/.\n",
    "\n",
    "\n",
    "**Software Requirements**:  \n",
    "\n",
    "The software and operating system versions used to develop this notebook are listed below. To avoid encountering issues related to version conflicts among Python packages, we recommend creating a new environment variable and installing the required packages specifically for this notebook.\n",
    "\n",
    "> fsspec    : 2024.6.0  \n",
    "geopandas : 0.14.4  \n",
    "numpy     : 1.26.4  \n",
    "matplotlib: 3.9.0  \n",
    "sys       : 3.12.3   \n",
    "dask      : 2024.5.2  \n",
    "xarray    : 2024.5.0  \n",
    "rioxarray : 0.15.5  \n",
    "geocube   : 0.5.2  \n",
    "s3fs      : 2024.6.0  \n",
    "zarr      : 2.18.2  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c353a2-226b-4dfb-8db4-c2fbcadb2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import dask\n",
    "import numpy\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79279a53-22ae-40cc-a05c-67188f18ccc4",
   "metadata": {},
   "source": [
    "We'll use `dask` to parallelize our code. This is a very powerful library that has been integrated into libraries such as `xarray` which enables us to use its capabilities without writing any parallel code. However, the process for writing parallel code using `dask` is straightforward and well documented, for more information see their website [here].(https://www.dask.org/).\n",
    "\n",
    "In this notebook, we'll be using `dask` to speed up our access of the AORC dataset. To visualize the progress of long running jobs, we'll first need to create a \"cluster.\" The cluster defines the number of workers and their respective computing resources. This should be scaled to the hardware that you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005eb820-57c2-4ad6-81e8-26ed37d4f075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(client.dashboard_link)\n",
    "except:    \n",
    "    # The client should be customized to your workstation resources.\n",
    "    # This is configured for a \"Large\" instance on ciroh.awi.2i2c.cloud\n",
    "    # client = Client()\n",
    "    client = Client(n_workers=8, memory_limit='10GB') # Large Machine\n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dc74c-01a2-4fef-bdfe-f021de978be4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bb9cd-cf11-4089-accb-fd50b1a99230",
   "metadata": {},
   "source": [
    "## Access the AORC Forcing Data using Xarray\n",
    "\n",
    "In this notebook we'll be working with AORC v1.1 meteorological forcing. These data are publicly available as part of the NOAA National Water Model v3.0 Retrospective archive on AWS registry of open data. These data are available in the `Zarr` format which offers a convienent and efficient means for slicing and subsetting very large datasets using libraries such as `xarray`. The following link will navigate you to the data, this can be helpful for understanding what data are available and how they are structured:\n",
    "\n",
    "Homepage  : https://registry.opendata.aws/nwm-archive/  \n",
    "Zarr Store: https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/index.html#CONUS/zarr/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf10c9-0c27-4bab-bd1f-97c130e805db",
   "metadata": {},
   "source": [
    "Define a few parameters for accessing the specific variable that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99a7fe-5000-4806-a7fc-0c118dfd045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_url = 's3://noaa-nwm-retrospective-3-0-pds'\n",
    "region = 'CONUS'\n",
    "variable = 'precip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf725cc-d6e0-49ed-832a-7e8bf50fc953",
   "metadata": {},
   "source": [
    "We'll use the `fsspec` library to load these data. The `fsspec` library provides a filesystem interface for data accessing remote data such as the AORC Zarr store on AWS. To learn more about `fsspec`, see their documentation [here](https://filesystem-spec.readthedocs.io/en/latest/). Since these data are stored in an S3 bucket, `fsspec` will leverage the `s3fs` package to provide a filesystem interface to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed17436-4bf9-4028-a035-36ee8eed386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the zarr store that we want\n",
    "s3path = f\"{bucket_url}/{region}/zarr/forcing/{variable}.zarr\"\n",
    "\n",
    "# load these data using xarray\n",
    "ds = xr.open_zarr(fsspec.get_mapper(s3path, anon=True), consolidated=True)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666761a-820d-40b0-b07c-6d6af47383b4",
   "metadata": {},
   "source": [
    "Notice that this loaded very fast. That's because it performed a \"lazy\" load of the data, i.e. only the metadata was loaded. Data values will not be accessed until computations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062fcc7-d2fd-4e48-a0c6-5140589cee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of ds={sys.getsizeof(ds)} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84744cd2-5a88-48d7-8101-d198b1148324",
   "metadata": {},
   "source": [
    "## Slicing and Visualizing the AORC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b95d0-da8a-4557-8236-54cab23a6366",
   "metadata": {},
   "source": [
    "Since this is a lot of data, let's reduce the size that we're looking at to a single timestep. We can do this in a number of ways, however in this case we're just selecting the first index of data. For more information on slicing data, see the xarray documentation [here](https://docs.xarray.dev/en/v2023.09.0/user-guide/indexing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928a60d-2626-478b-b930-9b67e30dde81",
   "metadata": {},
   "source": [
    "#### Slice these Data using Xarray Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b9f4d-c7af-4115-b265-b72ab5df3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice data using index locators\n",
    "ds_sel = ds.isel(time=0)\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb17da-77bf-4758-9589-0c03e9dbac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select using multiple locators\n",
    "ds_sel = ds.isel(time=0, x=1000, y=1000)\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1660f7c-2214-4528-a0bb-ca42696b1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAINRATE value associated with this slice of the AORC \n",
    "ds_sel.RAINRATE.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165ff43-f0bc-4ba4-9ccc-2cc4992b0542",
   "metadata": {},
   "source": [
    "Selecting a specific data point in time is not very useful. Often we're interested in a time series of data. Let's extend our previous example to collect data through a range of time. This can be done using indexing as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8d787-44b3-4fe8-bb55-c8ec7edacab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sel = ds.isel(time=range(0, 100), # use range to select multiple indices\n",
    "                 x=1000,\n",
    "                 y=1000)\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a3425-c875-4c90-85ce-ab6890f26147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries of data associated with the RAINRATE variable\n",
    "ds_sel.RAINRATE.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6af119-d0e0-4bd1-a626-8fa9f11875a6",
   "metadata": {},
   "source": [
    "We now have a timeseries at a single grid cell, but we defined the time range using array indexing. This makes it difficult to select a specific time range of interest. We can use datetime slicing instead of array indexing to acquire a more precise time range. First let's figure out the data range for which data is available by returning the minimum and maximum dates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f84081-2897-49a0-a6bd-99ac677c99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min = ds.time.min().values\n",
    "dt_max = ds.time.max().values\n",
    "print(f'The daterange of our data is {dt_min} - {dt_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d32ff-1f2a-4c1f-93e9-da750f2f27b2",
   "metadata": {},
   "source": [
    "Next we can slice our data for a time span within this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca52a5-ad3a-4b92-9303-4d0d18c05bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the spatial area of interest using array indexing\n",
    "ds_sel = ds.isel(x=1000,\n",
    "                 y=1000)\n",
    "\n",
    "# select the time span of interest using date range slicing\n",
    "ds_sel = ds_sel.sel(time=slice('2020-01-01', '2021-01-01'))\n",
    "\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff0c77-c5ff-4bbd-b64b-f63aba94b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries of data associated with the RAINRATE variable.\n",
    "ds_sel.RAINRATE.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a83d41-35aa-487b-9234-d2c1bb1865b9",
   "metadata": {},
   "source": [
    "Similarly we can modify our example to select a spatial range rather than a single grid cell. This can easily be done using array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555ad95-bb6e-4742-b97f-1f550dce9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the spatial area of interest using array indexing\n",
    "ds_sel = ds.isel(x=range(1000, 2000),\n",
    "                 y=range(1000,2000))\n",
    "\n",
    "# select the time span of interest using date range slicing\n",
    "ds_sel = ds_sel.sel(time=slice('2020-01-01', '2021-01-01'))\n",
    "\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7675f5c-59fc-4a6f-b155-a30d30190349",
   "metadata": {},
   "source": [
    "We now have 1000x1000 arrays of data stacked through time. Plotting becomes a bit more tricky here, but we can preview our data by plotting at a single time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f463c-f2a2-4778-84f4-a7bedd23f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a single time within our data cube\n",
    "rainrate = ds_sel.isel(time=5006).RAINRATE\n",
    "\n",
    "# plot the values where rainrate is greater than 0.0\n",
    "rainrate.where(rainrate > 0.0).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd674eef-bccb-45db-84f2-f2b3e04f9b5b",
   "metadata": {},
   "source": [
    "We can extend this example to select a spatial using coordinate values instead of array indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd7559-955e-428c-9795-26a87bb36c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymin = -846500.312\n",
    "ymax = -786500.312\n",
    "xmin = -1274499.125\n",
    "xmax = -426499.1875\n",
    "\n",
    "# select the time span and spatial area of interest using slicing\n",
    "ds_sel = ds_sel.sel(time=slice('2020-01-01', '2021-01-01'),\n",
    "                    y=slice(ymin, ymax),\n",
    "                    x=slice(xmin, xmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6c947-fe31-4c8a-8d0e-41853bd7e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot  a single time step within our data cube\n",
    "ds_sel.isel(time=5006).RAINRATE.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacabb76-4bd1-4210-b8a6-a5fc77edfa54",
   "metadata": {},
   "source": [
    "## Aligning Gridded AORC with Watershed Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcb234-4696-43f3-8e9a-718a137a94d6",
   "metadata": {},
   "source": [
    "Often times we are interested in gridded data that aligns with a vector area such as a watershed boundary. We can align the AORC gridded data on vectors using the geocube library. First, let's load a watershed Shapefile that defines our area of interest. `GeoPandas` makes working with Shapefiles in Python very easy and intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa391cb-d562-44b5-8e50-beb8a4a54bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the watershed shapefile\n",
    "gdf = gpd.read_file('sample-data/watershed.shp')\n",
    "\n",
    "# preview the watershed\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafa62f-322a-4bf7-a362-8e27361a9436",
   "metadata": {},
   "source": [
    "We can also preview the attributes of this shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f12a61-99b0-4807-a319-e2835df12022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c74f3-7295-4406-b7bc-679b252047c8",
   "metadata": {},
   "source": [
    "To align the gridded AORC data on these vector boundaries we need to first set the coordinate reference system (CRS) within the xarray dataset. It's CRS is defined in the metadata but it isn't set in a way that we can leverage it. Let's change that by using the rasterIO extension to xarray, called `rioxarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3fb06-dd53-4bb9-8783-ed43374fad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the crs in the dataset\n",
    "ds.rio.set_crs(ds.crs.attrs['esri_pe_string'])\n",
    "ds.rio.write_crs(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80de71-190b-4682-bcb1-a73d32097385",
   "metadata": {},
   "source": [
    "Next we need to make sure that the AORC CRS matches that of our watershed. If these don't align, we'll need to perform geospatial transformations before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8e036-5f62-4dde-8632-b9c3f11244be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'AORC CRS:\\n-----\\n{ds.rio.crs.to_proj4()}')\n",
    "print(f'\\nShapefile CRS:\\n-----\\n{gdf.crs.to_proj4()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16f116-3fa2-4795-adc2-8bc8947d98cd",
   "metadata": {},
   "source": [
    "Since these coordinate systems differ, we'll need to convert one of them so that they align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3d3df-9493-443f-a5c5-1a415c14830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the shapefile into the coordinate system of the xarray dataset\n",
    "gdf = gdf.to_crs(ds.rio.crs)\n",
    "print(f'\\nShapefile CRS:\\n-----\\n{gdf.crs.to_proj4()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960791c-8365-4bb9-8959-55d36ee30c1d",
   "metadata": {},
   "source": [
    "Let's clip the AORC data to the extent of this watershed. This can be done using `rioxarray`'s \"clip\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1e94e-c3b5-49c7-9f29-9ccd86c5bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip the data\n",
    "ds_sel = ds.rio.clip(\n",
    "         gdf.geometry.values,\n",
    "         gdf.crs,\n",
    "         all_touched=True,   # select all grid cells that touch the vector boundary\n",
    "         drop=True,          # drop anything that is outside the clipped region\n",
    "         invert=False,\n",
    "         from_disk=True)\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cc7e0-d9ad-4811-b7ff-2509bfe335a1",
   "metadata": {},
   "source": [
    "Preview our data at a single point in time. We'll use some `matplotlib` features to make a more interesting plot that contains both our gridded data as well as our vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72780cc6-f126-41d1-9ccd-dd47520c120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# add RAINRATE at a single time to the plot\n",
    "ds_sel.isel(time=5006).RAINRATE.plot(ax=ax)\n",
    "\n",
    "# add our watershed to the plot\n",
    "gdf.plot(ax=ax, facecolor='none', edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9e53a-ca24-4fe6-87c8-ff2b21d40f5e",
   "metadata": {},
   "source": [
    "We've clipped the AORC dataset to the extent of our watershed boundary, however it still has no relation to the individual subcatchments. To better connect these two datasets, we can create a new dataset variable that represents a mask of grid cells that are associated with each subcatchment. We'll use the `geocube` library to accomplish this task.\n",
    "\n",
    "Note that the method we're using will associate grid cell with the watershed that it overlaps the most with. There are more advanced ways to create a mapping using various interpolation methods that will distribute values cells across all watershed boundaries that they intersect with. This is left as a future exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba34dd6-7905-4dc9-9d82-c238f018a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create zonal id column\n",
    "gdf['cat'] = gdf.id.str.split('-').str[-1].astype(int)\n",
    "\n",
    "# select a single array of data to use as a template\n",
    "rainrate_data = ds_sel.isel(time=0).RAINRATE\n",
    "\n",
    "# create a grid for the geocube\n",
    "out_grid = make_geocube(\n",
    "    vector_data=gdf,\n",
    "    measurements=[\"cat\"],\n",
    "    like=ds_sel # ensure the data are on the same grid\n",
    ")\n",
    "\n",
    "# add the catchment variable to the original dataset\n",
    "ds_sel = ds_sel.assign_coords(cat = (['y','x'], out_grid.cat.data))\n",
    "\n",
    "# compute the unique catchment IDs which will be used to compute zonal statistics\n",
    "catchment_ids = numpy.unique(ds_sel.cat.data[~numpy.isnan(ds_sel.cat.data)])\n",
    "\n",
    "print(f'The dataset contains {len(catchment_ids)} catchments')\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c137b08-fe5b-455a-83c8-130bac45adaf",
   "metadata": {},
   "source": [
    "We can now select and plot data for spatial areas that correspond with out catchment identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8ddb5-a979-41ea-98a6-851634fc8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot RAINRATE for a single catchment\n",
    "ds_sel.isel(time=5006).cat.plot(ax=ax, levels=35, cmap='gist_ncar');\n",
    "\n",
    "# add our watershed to the plot\n",
    "gdf.plot(ax=ax, facecolor='none', edgecolor='k');\n",
    "\n",
    "# adjust the x and y limits of the plot so we can see the entire watershed.\n",
    "ax.set_xlim(ds_sel.x.min(), ds_sel.x.max())\n",
    "ax.set_ylim(ds_sel.y.min(), ds_sel.y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70330e7-6f7f-476f-96ac-dc0d07a1c484",
   "metadata": {},
   "source": [
    "We can plot data for a single catchment by filtering on it's catchment identifier. These identifers are defined by the geopandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67037a3-1541-430c-8d19-46359df23228",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abbcaa-9844-4653-8438-a9213f0d6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot RAINRATE for a single catchment\n",
    "cat_id=2853621\n",
    "ds_sel.isel(time=5006).where(ds_sel.cat==cat_id, drop=True).RAINRATE.plot(ax=ax);\n",
    "\n",
    "# add our watershed to the plot\n",
    "gdf.plot(ax=ax, facecolor='none', edgecolor='k');\n",
    "\n",
    "# adjust the x and y limits of the plot so we can see the entire watershed.\n",
    "ax.set_xlim(ds_sel.x.min(), ds_sel.x.max())\n",
    "ax.set_ylim(ds_sel.y.min(), ds_sel.y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbba82-ac17-4df6-a1e3-e320e3cf1b6a",
   "metadata": {},
   "source": [
    "We can now perform computations on AORC data that aligns with subcatchments. For example, let's plot the average precipitation rate for a single catchment through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0abe2-e073-4fde-b45b-5d26bdbe5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform spatial selection using the catchment id defined in the cell above.\n",
    "dat = ds_sel.where(ds_sel.cat==cat_id, drop=True)\n",
    "\n",
    "# compute mean rainrate across dimensions x and y.\n",
    "dat = dat.RAINRATE.mean(dim=['x','y'])\n",
    "\n",
    "# slice our dataset to a reasonable time range\n",
    "dat = dat.sel(time=slice('2020-01-01', '2020-06-01')).compute()  # triggers the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d23d7d-8aec-499a-8c6b-9cdc7cefe9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "dat.plot(ax=ax)\n",
    "ax.set_title(f'Mean Rainrate for CAT-{cat_id}')\n",
    "ax.set_xlabel('Time')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
