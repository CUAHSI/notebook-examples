{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b469fd7b-c0ad-48a2-9d7d-a7f1bf0658db",
   "metadata": {},
   "source": [
    "## Query AORC Forcing Data via HydroShare Thredds\n",
    "\n",
    "**Authors**: Tony Castronova <acastronova@cuahsi.org>, Irene Garousi-Nejad <igarousi@cuahsi.org>  \n",
    "**Last Updated**: 03.28.2023\n",
    "\n",
    "**Description**:  \n",
    "\n",
    "This example demonstrates how to collect and visualize AORC forcing data from HydroShare's THREDDS server, however the process is the same for accessing these data in other THREDDs instances as well.\n",
    "\n",
    "> The Analysis of Record for Calibration (AORC) is a gridded record of near-surface weather conditions covering the continental United States and Alaska and their hydrologically contributing areas. It is defined on a latitude/longitude spatial grid with a mesh length of ~800 m (30 arc seconds), and a temporal resolution of one hour. Elements include hourly total precipitation, temperature, specific humidity, terrain-level pressure, downward longwave and shortwave radiation, and west-east and south-north wind components. It spans the period from 1979 at Continental U.S. (CONUS) locations / 1981 in Alaska, to the near-present (at all locations). This suite of eight variables is sufficient to drive most land-surface and hydrologic models and is used to force the calibration run of the National Water Model (NWM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e0261-0fd8-4011-bce8-90598fb8dc44",
   "metadata": {},
   "source": [
    "**Software Requirements**\n",
    "\n",
    "This notebook was developed using the following software and operating system versions.\n",
    "\n",
    "OS: MacOS Ventura 13.0.1  \n",
    "Python: 3.10.0\n",
    "Zarr: 2.13.2  \n",
    "NetCDF4: 1.6.1  \n",
    "xarray: 0.17.0  \n",
    "fsspec: 0.8.7  \n",
    "dask: 2021.3.0  \n",
    "hvplot: 0.7.1  \n",
    "holoviews: 1.14.2  \n",
    "pynhd: 0.10.1\n",
    "nest-asyncio: 1.5.6\n",
    "\n",
    "\n",
    "The following commands should help you set up these dependencies\n",
    "```\n",
    "$ conda create -n nwm-env python=3.10.0\n",
    "\n",
    "$ conda install -y -c pyviz -c conda-forge pynhd folium s3fs hvplot dask distributed zarr\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dadef-1b89-4fa5-bac5-89baa70c2f51",
   "metadata": {},
   "source": [
    "## 1. Search AORC Forcing on HydroShare Thredds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473a255-3a76-43e2-8251-9c859182994d",
   "metadata": {},
   "source": [
    "The AORC Forcing data used in this notebook covers the Great Basin watershed from 2010-2019 and is stored in HydroShare's Thredds catalog:\n",
    "\n",
    "https://thredds.hydroshare.org/thredds/catalog/aorc/data/16/catalog.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb325c4-329d-494b-a1eb-9a439b5bc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import xarray\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40f285-867a-4664-b525-0ebb463d7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f63ef6-1d24-41bd-9681-9d62829fad6c",
   "metadata": {},
   "source": [
    "To discover the available data, we're reading an XML file and parsing its content. Using this information we create a list of URLs that can be loaded using `xarray` in later steps. This process is optional and can be bypassed by manually collecting these urls from the THREDDs interface via web browser. \n",
    "\n",
    "Identify the files in the catalog via the catalog.xml document. The following link provides a list of all available data for the HUC-2 region for the Great Basin; `HUC=16`. \n",
    "https://thredds.hydroshare.org/thredds/catalog/aorc/data/16/catalog.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15f448-6022-4db4-be70-a3c8d8ff91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_base_url = 'https://thredds.hydroshare.org/thredds/catalog'\n",
    "dods_base_url = 'https://thredds.hydroshare.org/thredds/dodsC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a4ca4-aa26-46ae-8f1b-eefb819a0872",
   "metadata": {},
   "source": [
    "Read the catalog.xml document and extract all `urlPath` attributes. We'll use the `urlPath` attribute to build the complete path to each file we want to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a9ab4-a463-4081-a1a6-0c522f07c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{catalog_base_url}/aorc/data/16/catalog.xml'\n",
    "root = ET.fromstring(requests.get(url).text)\n",
    "ns = '{http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85698f-43c5-48f1-8d70-8ec43a9771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xpath top select all \"dataset\" elements.\n",
    "elems = root.findall(f'.//{ns}dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130df3d-a208-4ddf-add5-f54ad37c257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through results and extract the \"urlPath\" attribute values\n",
    "paths = []\n",
    "for elem in elems:\n",
    "    atts = elem.attrib\n",
    "    if 'urlPath' in atts.keys():\n",
    "        paths.append(f\"{dods_base_url}/{atts['urlPath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b80efb-c3ac-42cb-95a5-7f33fd284691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to isolate only files that end with \".nc\"\n",
    "paths = list(filter(re.compile(\"^.*\\.nc$\").match, paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2c6f1-6c4a-4367-a3db-6db534a69243",
   "metadata": {},
   "source": [
    "Print out some information about the files that we've found. For example, the total number of files as well as the names for the first and last files. The names of the first and last files will give us the temporal range of data that's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9153310-25b5-4472-8bbb-09928fbc4dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {len(paths)} individual files')\n",
    "print(f'The first file is named: {os.path.basename(paths[0])}')\n",
    "print(f'The last file is named: {os.path.basename(paths[-1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d5bb7-9bad-4a31-9b92-ae8cf52a47c6",
   "metadata": {},
   "source": [
    "## 2. Preview Data From a Single File\n",
    "\n",
    "Connect to the THREDDs server and load a single month of data to explore that variables and spatial extent that is available to us. Using the URLs stored in the `paths` variable, we can load the first file via index 0 (i.e. `paths[0]`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df34c9-ccb5-4a5a-a202-c8d697bff2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset(paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11c0ba-79b6-4eb0-b2a4-f099de07e46a",
   "metadata": {},
   "source": [
    "Display the variables that are contained in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798a8e0-443c-4547-814f-a6afd07433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c165e32-4847-4a88-bd72-96ded68c58fd",
   "metadata": {},
   "source": [
    "Plot Rainrate for the first timestep across the entire grid domain. There are numerous ways to select data within an Xarray DataSet, for more information on see: https://docs.xarray.dev/en/stable/user-guide/indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc28095-38ae-4736-8689-f1af76f518af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot RAINRATE for the entire spatial grid at Time=0\n",
    "ds.isel(Time=0).RAINRATE.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6863f9-b6e9-4407-8fa1-df86dadfa26b",
   "metadata": {},
   "source": [
    "Plot RAINRATE through time (50 timesteps) at a single grid cell in the domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527c750-8b41-4b81-9677-0d09115b0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(Time=range(0, 50), south_north=1, west_east=1).RAINRATE.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b135b79-eaa5-462c-b504-6727d94714fd",
   "metadata": {},
   "source": [
    "## 3. Access Data in Multiple Files\n",
    "\n",
    "To access data through time ranges longer than 1 month we'll need to access multiple files. For example, for the time range 01/01/2010 - 03/23/2010 we'll need to load three files:\n",
    "\n",
    "- https://thredds.hydroshare.org/thredds/dodsC/aorc/data/16/201001.nc\n",
    "- https://thredds.hydroshare.org/thredds/dodsC/aorc/data/16/201002.nc\n",
    "- https://thredds.hydroshare.org/thredds/dodsC/aorc/data/16/201003.nc\n",
    "\n",
    "When loading large amounts of data (e.g. each of these file is 16 GiB), Dask chunking becomes extremely important. Each of the files that we're accessing contains ~744 timesteps and ~700,000 grid cells (820 rows, 855 columns), which is approximately 500 million elements. The Dask chunking documentation suggests:\n",
    "\n",
    "   > A good rule of thumb is to create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 matrix). With large arrays (10+ GB), the cost of queueing up Dask operations can be noticeable, and you may need even larger chunksizes.\n",
    "   > https://docs.xarray.dev/en/stable/user-guide/dask.html\n",
    "   \n",
    "We've found that following chunking scheme provides adequate performance for many applications:\n",
    "\n",
    "|Dimension|Chunks|\n",
    "|---|---|\n",
    "|Time| 10 |\n",
    "|west_east|285|\n",
    "|south_north|275|\n",
    "\n",
    "The results in chunks that contain approximately 800,000 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69247924-b31c-4f94-a344-bba5a409fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiple files using open_mfdataset\n",
    "ds = xarray.open_mfdataset(paths[0:5],\n",
    "                           concat_dim='Time',\n",
    "                           combine='nested',\n",
    "                           parallel=True,\n",
    "                           chunks={'Time': 10, 'west_east': 285, 'south_north':275})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509d43b-430c-4a05-97e1-822fbf2b98f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Preview the data, notice there are 3624 timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd49fa9-ded8-4cc2-9182-9eaae20aabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d5517-367d-4831-8bb7-b46a01eec396",
   "metadata": {},
   "source": [
    "Plot `LWDOWN` for a time range that spans more than one month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb61d7b-aa20-4d92-806e-9a65fced1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(Time=range(600, 1000), south_north=1, west_east=1).LWDOWN.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b33b4-74f2-4ca3-862b-9c63b610deda",
   "metadata": {},
   "source": [
    "Create a coordinate containing datetime values so that we can perform queries using human readable datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036b05b-2905-434b-8d47-33d96052304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data by valid_time\n",
    "ds = ds.sortby('valid_time')\n",
    "\n",
    "# create coordinate to allow loc searches\n",
    "ds = ds.assign_coords(Time=('Time', ds.valid_time.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9893dfe-dc9a-485c-bddd-410cb80c252c",
   "metadata": {},
   "source": [
    "Slice the dataset using a human readable time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbfc8e-e32e-4bfb-bd9c-e96ca6c1732d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.loc[dict(Time=slice('2010-01-01','2010-01-03'), west_east=1, south_north=1)].LWDOWN.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351750f-1f9f-4bfb-b16c-c48443830eaf",
   "metadata": {},
   "source": [
    "## 4. Advanced Usage\n",
    "\n",
    "The following demonstrates some of the more advanced computations and visualization that you can perform. Start by loading the entire 10-year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8956d2c-245c-4fc6-b0fb-0dad7c4955b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "# load multiple files using open_mfdataset\n",
    "ds = xarray.open_mfdataset(paths,\n",
    "                           concat_dim='Time',\n",
    "                           combine='nested',\n",
    "                           parallel=True,\n",
    "                           chunks={'Time': 10, 'west_east': 285, 'south_north':275})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41146c61-9e2b-466a-bb10-7f275256515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort data by valid_time\n",
    "ds = ds.sortby('valid_time')\n",
    "\n",
    "# create coordinate to allow loc searches\n",
    "ds = ds.assign_coords(Time=('Time', ds.valid_time.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc535f6b-6b76-4018-a656-1794d9fa58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cc26b-8889-46d4-a7ea-ebdf3005df43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot the daily average `LWDOWN` for a period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428800d-3c93-4df1-962c-d906050c9e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = ds.loc[dict(Time=slice('2010-01-01','2010-01-5'), west_east=range(100, 150), south_north=range(100, 150))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964b8bb-ab07-4114-a858-2366123c43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.LWDOWN.resample(Time='1d').mean(['west_east', 'south_north']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907c1b8-7a97-4448-80c8-450e1a6e142b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot animation of `LWDOWN` through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f21480-93a3-4d6a-8f1d-c76b7549e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot needed to plot the dataset, but animation only needed much further down.\n",
    "from matplotlib import pyplot as plt, animation\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed to display graphics calculated outside of jupyter notebook\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bc92f-c525-4875-9142-8a5801867c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = '2010-01-01'\n",
    "et = '2010-01-05'\n",
    "\n",
    "dat = ds.loc[dict(Time=slice(st, et), west_east=range(100, 150), south_north=range(100, 150))].LWDOWN.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4d61b-3825-445c-ac5c-3eb0f8747485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle on the figure and the axes\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Plot the initial frame. \n",
    "cax = dat[0].plot(\n",
    "    add_colorbar=True,\n",
    "    cmap='coolwarm',\n",
    "    vmin=0, vmax=500,\n",
    "    cbar_kwargs={\n",
    "        'extend':'neither'\n",
    "    }\n",
    ")\n",
    "\n",
    "# create a list of datetimes to update the figure title\n",
    "start = datetime.strptime(st, '%Y-%m-%d') \n",
    "datetimes = [start + timedelta(hours=i) for i in range(0, len(dat))]\n",
    "\n",
    "# Next we need to create a function that updates the values for the colormesh, as well as the title.\n",
    "def animate(frame):\n",
    "    cax.set_array(\n",
    "        dat[frame].values.flatten()\n",
    "    )\n",
    "    ax.set_title(f\"Time = {datetimes[frame].strftime('%m-%d-%Y %H:%M')}\")\n",
    "\n",
    "# Finally, we use the animation module to create the animation.\n",
    "ani = animation.FuncAnimation(\n",
    "    fig,             \n",
    "    animate,         \n",
    "    frames=len(dat),\n",
    "    interval=200     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93260229-8edc-43e8-a865-80126e93f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nwm-env]",
   "language": "python",
   "name": "conda-env-nwm-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
